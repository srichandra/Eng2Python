{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "EnglishToPython_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yt02dovzBrGe",
        "lasting-motorcycle",
        "dental-paper",
        "increased-privilege",
        "judicial-punch",
        "focused-workplace"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "congressional-siemens"
      },
      "source": [
        "## English to Python code\n",
        "\n",
        "Steps taken:\n",
        "\n",
        "1. Read/Clean the dataset. Dataset consists of english sentence as a comment followed by python code corresponding to the sentence. The data required lot of cleaning which will be discussed now along with the reading.  Read the dataset line by line to extract the pairs of english sentence and corresponding python block. Difficulty in extraction comes from the fact that the english sentences start from \"#\" as well all the comments in python. We create an ignore pattern to first filter out lines that contain In[00]: or just #[num] or just bunch of \\s or contain word \"driver block/code\". Also, we use a minimum length based on the histogram of english sentence to filter out the comments. \n"
      ],
      "id": "congressional-siemens"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt02dovzBrGe"
      },
      "source": [
        "### Importing Libs "
      ],
      "id": "yt02dovzBrGe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owQtF0BXrLcx",
        "outputId": "8bd07503-5163-4b43-c4ce-9ea0248082eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "owQtF0BXrLcx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ-5SjhKrX42"
      },
      "source": [
        "!cp  drive/MyDrive/NLP/english_python_data_cleaned.txt ."
      ],
      "id": "YZ-5SjhKrX42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.091580Z",
          "start_time": "2021-03-11T09:43:05.251454Z"
        },
        "hidden": true,
        "id": "saved-throat"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import os, sys, re\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import widgets, interact\n",
        "import cgi, string, sys, io\n",
        "import keyword, token, tokenize\n",
        "import spacy\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Dataset\n",
        "from torchtext import data\n",
        "from torchtext.legacy.data import BucketIterator, Field, Example\n",
        "import math, time, random\n",
        "from collections import defaultdict\n",
        "from torchtext.data.metrics import bleu_score"
      ],
      "id": "saved-throat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.095639Z",
          "start_time": "2021-03-11T09:43:06.093575Z"
        },
        "hidden": true,
        "id": "impaired-sustainability"
      },
      "source": [
        "Path.ls = lambda x: list(x.iterdir())"
      ],
      "id": "impaired-sustainability",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.113514Z",
          "start_time": "2021-03-11T09:43:06.097188Z"
        },
        "hidden": true,
        "id": "recent-probe"
      },
      "source": [
        "ignore_pattern = re.compile(r'^\\s*#+\\s*[0-9]*\\s*\\n|^\\s*\\n+\\s*$|\\s*#!.*|^\\s*#\\s+in\\[[0-9]+\\]|^\\s*#\\s+driver')\n"
      ],
      "id": "recent-probe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.132441Z",
          "start_time": "2021-03-11T09:43:06.115165Z"
        },
        "hidden": true,
        "id": "authorized-fiction"
      },
      "source": [
        "device = 'cuda'"
      ],
      "id": "authorized-fiction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "lasting-motorcycle"
      },
      "source": [
        "### Reading Data"
      ],
      "id": "lasting-motorcycle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.147540Z",
          "start_time": "2021-03-11T09:43:06.134825Z"
        },
        "hidden": true,
        "id": "attempted-overhead"
      },
      "source": [
        "datapath = Path(os.getcwd())"
      ],
      "id": "attempted-overhead",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.334621Z",
          "start_time": "2021-03-11T09:43:06.150493Z"
        },
        "hidden": true,
        "id": "thermal-threat"
      },
      "source": [
        "# empty variables to store the english sentences and python codeblocks\n",
        "english_sents = []\n",
        "python_codes = []\n",
        "ignored_lines = []\n",
        "removed_comments = []\n",
        "minimum_length = 30\n",
        "# open the file to read\n",
        "with open(datapath/'english_python_data_cleaned.txt', 'r') as f:\n",
        "    # flag to check for the first code block\n",
        "    start_extraction = False\n",
        "    python_block = []\n",
        "    all_lines = f.readlines()\n",
        "    # remove certain lines from the data set\n",
        "    clean_lines = []\n",
        "    for line in all_lines:\n",
        "        if ignore_pattern.search(line.lower()):\n",
        "            continue\n",
        "        elif re.search(r'^\\s+#\\s*[0-9]*\\s*write',line.lower()):\n",
        "            clean_lines.append(re.sub(r'^\\s+#','#',line))\n",
        "        else:\n",
        "            clean_lines.append(line)\n",
        "    \n",
        "    # walk through each line by line\n",
        "    for i, line in enumerate(clean_lines):\n",
        "        if re.search(r'^#.*',line.lower()):\n",
        "            if len(line)<minimum_length and not re.search(r'^\\s*#\\s*[0-9]+\\s*\\w+',line):\n",
        "                continue\n",
        "            if len(python_block)>0:\n",
        "                python_codes.append(''.join(python_block))\n",
        "            elif start_extraction:\n",
        "                if re.search(r'write', line.lower()):\n",
        "                    english_sents[-1]+='\\n'\n",
        "                    english_sents[-1]+=line\n",
        "                elif len(english_sents[-1])<minimum_length:\n",
        "                    english_sents[-1]+='\\n' \n",
        "                    english_sents[-1]+=line\n",
        "                else:\n",
        "                    removed_comments.append(line)\n",
        "                continue\n",
        "                \n",
        "            if not start_extraction: \n",
        "                start_extraction = True\n",
        "            english_sents.append(line)\n",
        "            assert len(english_sents) == len(python_codes)+1,f\"{len(english_sents)}, {len(python_codes)}, {english_sents[-1]}\"\n",
        "            python_block = []\n",
        "        else:\n",
        "            if not start_extraction:\n",
        "                continue\n",
        "            python_block.append(line)\n",
        "    if len(python_block)>0:\n",
        "        python_codes.append(''.join(python_block))\n",
        "assert len(english_sents) == len(python_codes)"
      ],
      "id": "thermal-threat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.338788Z",
          "start_time": "2021-03-11T09:43:06.335837Z"
        },
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "secure-charleston",
        "outputId": "82cd512e-46b9-498e-a969-d29a4e1f872b"
      },
      "source": [
        "print(f\"Number of python programs to learn from --> {len(python_codes)}\")\n",
        "print(f\"Number of comments removed --> {len(removed_comments)}\")"
      ],
      "id": "secure-charleston",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of python programs to learn from --> 4411\n",
            "Number of comments removed --> 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "dental-paper"
      },
      "source": [
        "### Cleaning data"
      ],
      "id": "dental-paper"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.359806Z",
          "start_time": "2021-03-11T09:43:06.341557Z"
        },
        "hidden": true,
        "id": "ambient-favor"
      },
      "source": [
        "# checking if data is clean or not\n",
        "english_sents_cleaned = []\n",
        "for line in english_sents:\n",
        "    temp = line.split('\\n')\n",
        "    if len(temp)>2:\n",
        "        print(temp)\n",
        "    else:\n",
        "        english_sents_cleaned.append(line)"
      ],
      "id": "ambient-favor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.376251Z",
          "start_time": "2021-03-11T09:43:06.364320Z"
        },
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "experimental-truck",
        "outputId": "81baf3b4-678b-49b7-fe1e-931645e33964"
      },
      "source": [
        "# measure the length of the english sentence \n",
        "english_sent_lens = np.array([len(l) for l in english_sents_cleaned])\n",
        "# get the 5 percentile \n",
        "q5 = np.percentile(english_sent_lens, [5])\n",
        "print(q5)\n",
        "# if the 5% is less than 30-40 characters then fix the data"
      ],
      "id": "experimental-truck",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[40.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.426842Z",
          "start_time": "2021-03-11T09:43:06.379379Z"
        },
        "hidden": true,
        "id": "invisible-trouble"
      },
      "source": [
        "# get rid of comments from python code\n",
        "python_codes_cleaned = []\n",
        "for codeblock in python_codes:\n",
        "    tempblock = []\n",
        "    for line in codeblock.split('\\n'):\n",
        "        if re.search(r'^\\s*#', line):\n",
        "            continue\n",
        "        else:\n",
        "            tempblock.append(line)\n",
        "    python_codes_cleaned.append('\\n'.join(tempblock))\n",
        "        "
      ],
      "id": "invisible-trouble",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.485820Z",
          "start_time": "2021-03-11T09:43:06.427946Z"
        },
        "hidden": true,
        "scrolled": true,
        "id": "combined-first"
      },
      "source": [
        "# take note of how many newlines and spaces are there in every line\n",
        "# fixing indentation\n",
        "python_indent_fixed = []\n",
        "for i,codeblock in enumerate(python_codes_cleaned):\n",
        "    flag = False\n",
        "    tempblock = []\n",
        "    for line in codeblock.split('\\n'):\n",
        "        if re.search(r'^\\s+', line):\n",
        "            span = re.search(r'^\\s+', line).span()\n",
        "            if span[1]%4!=0:\n",
        "                needed_spaces = int(np.round(span[1]/4.))\n",
        "                line = ' '+line\n",
        "                #flag = True\n",
        "        tempblock.append(line.rstrip())\n",
        "    if flag:\n",
        "        print(i)\n",
        "        print(codeblock)\n",
        "        print('\\n'.join(tempblock))\n",
        "        break\n",
        "    python_indent_fixed.append('\\n'.join(tempblock))\n",
        "    "
      ],
      "id": "combined-first",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "increased-privilege"
      },
      "source": [
        "### Tokenizing"
      ],
      "id": "increased-privilege"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.493227Z",
          "start_time": "2021-03-11T09:43:06.487544Z"
        },
        "hidden": true,
        "id": "outdoor-island"
      },
      "source": [
        "class PyTokenizer:\n",
        "    \n",
        "    def __init__(self, raw):\n",
        "        self.raw = raw.expandtabs().strip()\n",
        "    def printme(self):\n",
        "        print(self.raw)\n",
        "    def tokenize(self):\n",
        "        # self.lines store the start of a newline\n",
        "        self.lines = [0, 0]\n",
        "        pos = 0\n",
        "        while 1:\n",
        "            pos = self.raw.find('\\n', pos) + 1\n",
        "            if not pos: break\n",
        "            self.lines.append(pos)\n",
        "        self.lines.append(len(self.raw))\n",
        "        # Parse the source and write it\n",
        "        self.pos = 0\n",
        "        text = io.StringIO(self.raw)\n",
        "        stringtokens = []\n",
        "        tokentypes = []\n",
        "        cleantokens = []\n",
        "        for tok in tokenize.generate_tokens(text.readline):\n",
        "            stringtokens.append(tok.string)\n",
        "            tokentypes.append(tok.type)\n",
        "            if len(tok.string)==0 or tok.string=='\\n' or tok.type==5:\n",
        "                if tok.type==56:\n",
        "                    cleantokens.append(token.tok_name[5])\n",
        "                else:\n",
        "                    cleantokens.append(token.tok_name[tok.type])\n",
        "            elif tok.type==55:\n",
        "                continue\n",
        "            else:\n",
        "                cleantokens.append(tok.string)\n",
        "        return cleantokens\n",
        "    "
      ],
      "id": "outdoor-island",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.508287Z",
          "start_time": "2021-03-11T09:43:06.494754Z"
        },
        "hidden": true,
        "id": "shared-washer"
      },
      "source": [
        "trial = python_indent_fixed[10]"
      ],
      "id": "shared-washer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.522518Z",
          "start_time": "2021-03-11T09:43:06.509891Z"
        },
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "intensive-better",
        "outputId": "b86bcce0-0dd3-4dbd-ff63-5faac2426933"
      },
      "source": [
        "mtokenizer = PyTokenizer(trial)\n",
        "mtokenizer.printme()"
      ],
      "id": "intensive-better",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def two_power(terms):\n",
            "    result = list(map(lambda x: 2 ** x, range(terms)))\n",
            "    print(f\"The total terms are: {terms}\")\n",
            "    for i in range(terms):\n",
            "        print(f\"2^{i} = {result[i]}\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.539667Z",
          "start_time": "2021-03-11T09:43:06.524699Z"
        },
        "hidden": true,
        "scrolled": true,
        "id": "worthy-accounting"
      },
      "source": [
        "#mtokenizer.tokenize()"
      ],
      "id": "worthy-accounting",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.950266Z",
          "start_time": "2021-03-11T09:43:06.542792Z"
        },
        "hidden": true,
        "id": "floppy-terrorist"
      },
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "id": "floppy-terrorist",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.954362Z",
          "start_time": "2021-03-11T09:43:06.951637Z"
        },
        "hidden": true,
        "id": "burning-herald"
      },
      "source": [
        "def tokenizer_en(text):\n",
        "    text = re.sub(r'#','',text).strip()\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenizer_py(text):\n",
        "    ptokenizer = PyTokenizer(text)\n",
        "    return ptokenizer.tokenize()\n",
        "    "
      ],
      "id": "burning-herald",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "judicial-punch"
      },
      "source": [
        "### Creating Dataset"
      ],
      "id": "judicial-punch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.967987Z",
          "start_time": "2021-03-11T09:43:06.955869Z"
        },
        "id": "religious-amount"
      },
      "source": [
        "SRC = Field(tokenize=tokenizer_en, lower=True, init_token='<sos>', eos_token='<eos>', batch_first=True)\n",
        "TRG = Field(tokenize=tokenizer_py, lower=True, init_token='<sos>', eos_token='<eos>', batch_first=True)"
      ],
      "id": "religious-amount",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:06.980054Z",
          "start_time": "2021-03-11T09:43:06.969699Z"
        },
        "id": "mysterious-resource"
      },
      "source": [
        "fields = [('src', SRC), ('trg', TRG)]"
      ],
      "id": "mysterious-resource",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.017323Z",
          "start_time": "2021-03-11T09:43:06.981996Z"
        },
        "scrolled": true,
        "id": "focal-programming"
      },
      "source": [
        "examples = []\n",
        "for i, (s,t) in enumerate(zip(english_sents_cleaned, python_indent_fixed)):\n",
        "    try:\n",
        "        examples.append(Example.fromlist([s,t],fields))\n",
        "    except (tokenize.TokenError, IndentationError) as ex:\n",
        "        print(i, ex)\n"
      ],
      "id": "focal-programming",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.020937Z",
          "start_time": "2021-03-11T09:43:08.018501Z"
        },
        "id": "turned-jordan"
      },
      "source": [
        "ds = Dataset(examples, fields)"
      ],
      "id": "turned-jordan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.035423Z",
          "start_time": "2021-03-11T09:43:08.022319Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careful-pantyhose",
        "outputId": "e6aa1ece-586b-4fb8-927c-7dafdd07293e"
      },
      "source": [
        "print(re.sub('newline', '\\n', ''.join(vars(ds[0])['trg'])))"
      ],
      "id": "careful-pantyhose",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num1=1.5\n",
            "num2=6.3\n",
            "sum=num1+num2\n",
            "print(f'sum: {sum}')\n",
            "endmarker\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.054454Z",
          "start_time": "2021-03-11T09:43:08.036957Z"
        },
        "scrolled": true,
        "id": "optimum-hawaii"
      },
      "source": [
        "train_ds, valid_ds = ds.split(split_ratio=0.8)"
      ],
      "id": "optimum-hawaii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.070774Z",
          "start_time": "2021-03-11T09:43:08.056695Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "closing-philadelphia",
        "outputId": "7efdcd19-85f3-4d68-f7fb-08be6be92125"
      },
      "source": [
        "len(train_ds), len(valid_ds)"
      ],
      "id": "closing-philadelphia",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3529, 882)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXMK85_cr4_n"
      },
      "source": [
        "train_iterator, valid_iterator = BucketIterator.splits((train_ds, valid_ds), batch_size = 32, sort_key = lambda x: len(x.src),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "id": "pXMK85_cr4_n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "focused-workplace"
      },
      "source": [
        "### Build Vocab"
      ],
      "id": "focused-workplace"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.168416Z",
          "start_time": "2021-03-11T09:43:08.103515Z"
        },
        "hidden": true,
        "id": "cross-warehouse"
      },
      "source": [
        "SRC.build_vocab(train_ds, min_freq=2)\n",
        "TRG.build_vocab(train_ds, min_freq=2)"
      ],
      "id": "cross-warehouse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.174988Z",
          "start_time": "2021-03-11T09:43:08.170441Z"
        },
        "hidden": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "successful-trinity",
        "outputId": "b07ae396-dc98-4f6a-bd57-bcadd8751dd0"
      },
      "source": [
        "len(SRC.vocab), len(TRG.vocab)"
      ],
      "id": "successful-trinity",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1454, 3615)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "general-tissue"
      },
      "source": [
        "### Model"
      ],
      "id": "general-tissue"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.193597Z",
          "start_time": "2021-03-11T09:43:08.176877Z"
        },
        "id": "banner-sampling"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #pos = [batch size, src len]\n",
        "        \n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src               "
      ],
      "id": "banner-sampling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.213101Z",
          "start_time": "2021-03-11T09:43:08.195620Z"
        },
        "id": "unlikely-naples"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src"
      ],
      "id": "unlikely-naples",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.239354Z",
          "start_time": "2021-03-11T09:43:08.215572Z"
        },
        "id": "naughty-pastor"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention"
      ],
      "id": "naughty-pastor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.259426Z",
          "start_time": "2021-03-11T09:43:08.241781Z"
        },
        "id": "lonely-electric"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        x = self.fc_2(x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "id": "lonely-electric",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.283612Z",
          "start_time": "2021-03-11T09:43:08.261939Z"
        },
        "id": "enormous-peeing"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 1000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "                \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "                            \n",
        "        #pos = [batch size, trg len]\n",
        "            \n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "\n",
        "                \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        output = self.fc_out(trg)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "            \n",
        "        return output, attention"
      ],
      "id": "enormous-peeing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.305921Z",
          "start_time": "2021-03-11T09:43:08.286150Z"
        },
        "id": "different-stake"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "            \n",
        "        #encoder attention\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "                    \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "id": "different-stake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.329442Z",
          "start_time": "2021-03-11T09:43:08.308655Z"
        },
        "id": "communist-carbon"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        \n",
        "        #trg = [batch size, trg len]\n",
        "        \n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
        "        \n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #trg_sub_mask = [trg len, trg len]\n",
        "            \n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        \n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #trg = [batch size, trg len]\n",
        "                \n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        \n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        \n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "        \n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "                \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        #output = [batch size, trg len, output dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return output, attention"
      ],
      "id": "communist-carbon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hired-lawyer"
      },
      "source": [
        "### Training utils"
      ],
      "id": "hired-lawyer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.346178Z",
          "start_time": "2021-03-11T09:43:08.332035Z"
        },
        "id": "humanitarian-medication"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim()>1:\n",
        "        nn.init.xavier_normal_(m.weight.data)\n",
        "    "
      ],
      "id": "humanitarian-medication",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.362055Z",
          "start_time": "2021-03-11T09:43:08.348899Z"
        },
        "id": "instrumental-accommodation"
      },
      "source": [
        "def count_parameters(model):\n",
        "    total_params = 0\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            total_params+=p.numel()\n",
        "    print(f\"Model has {total_params:,} trainable parameters\")"
      ],
      "id": "instrumental-accommodation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.382713Z",
          "start_time": "2021-03-11T09:43:08.364591Z"
        },
        "id": "solid-mercury"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "        if isinstance(criterion, nn.CTCLoss):\n",
        "            output_lengths = output.shape[1]\n",
        "            target_lengths = trg.shape[1]-1\n",
        "            output = output.contiguous().permute(1,0,2)\n",
        "            trg = trg[:,1:].contiguous()\n",
        "            loss = criterion(output, trg, output_lengths, target_lengths)\n",
        "        else:\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "                \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "            \n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "id": "solid-mercury",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.401886Z",
          "start_time": "2021-03-11T09:43:08.385405Z"
        },
        "id": "angry-appeal"
      },
      "source": [
        "def evaluate(model, iterator, criterion, metrics=None):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            if isinstance(criterion, nn.CTCLoss):\n",
        "                output_lengths = torch.full(size=(output.shape[0],), fill_value = output.shape[1], dtype=torch.long)\n",
        "                target_lengths = trg.shape[1]-1\n",
        "                output = output.contiguous().permute(1,0,2)\n",
        "                trg = trg[:,1:].contiguous()\n",
        "                loss = criterion(output, trg, output_lengths, target_lengths)\n",
        "\n",
        "            else:\n",
        "                output = output.contiguous().view(-1, output_dim)\n",
        "                trg = trg[:,1:].contiguous().view(-1)\n",
        "                loss = criterion(output, trg)\n",
        "            # compute bleu score\n",
        "            #if metrics is not None:\n",
        "                \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "id": "angry-appeal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:08.417249Z",
          "start_time": "2021-03-11T09:43:08.404413Z"
        },
        "id": "peaceful-military"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "id": "peaceful-military",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "6bBnDm2Ab5wp",
        "outputId": "53b860d3-a0b5-44c4-f9ab-e95c90abd42b"
      },
      "source": [
        "candidate_corpus = [[0, 0, 'pytorch', 'test'], ['Another', 'Sentence']]\n",
        "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
        "bleu_score(candidate_corpus, references_corpus)\n"
      ],
      "id": "6bBnDm2Ab5wp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-48959c3cf82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcandidate_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Another'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreferences_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'My'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pytorch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Completely'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Different'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/metrics.py\u001b[0m in \u001b[0;36mbleu_score\u001b[0;34m(candidate_corpus, references_corpus, max_n, weights)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mreference_counters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_counters\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0m_compute_ngram_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcandidate_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_ngram_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mclipped_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_counter\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mreference_counters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/metrics.py\u001b[0m in \u001b[0;36m_compute_ngram_counter\u001b[0;34m(tokens, max_n)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmax_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     ngrams_counter = collections.Counter(tuple(x.split(' '))\n\u001b[0;32m---> 30\u001b[0;31m                                          for x in ngrams_iterator(tokens, max_n))\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mngrams_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fast path when counter is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/data/metrics.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmax_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     ngrams_counter = collections.Counter(tuple(x.split(' '))\n\u001b[0;32m---> 30\u001b[0;31m                                          for x in ngrams_iterator(tokens, max_n))\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mngrams_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dried-plymouth"
      },
      "source": [
        "### Training Model\n"
      ],
      "id": "dried-plymouth"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:10.401261Z",
          "start_time": "2021-03-11T09:43:08.419946Z"
        },
        "id": "colored-annotation"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.2\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "id": "colored-annotation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:10.405444Z",
          "start_time": "2021-03-11T09:43:10.402396Z"
        },
        "id": "australian-thanksgiving",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13de483-e8ac-49a6-b564-cf946f5322fa"
      },
      "source": [
        "count_parameters(model)"
      ],
      "id": "australian-thanksgiving",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model has 6,692,383 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:10.418849Z",
          "start_time": "2021-03-11T09:43:10.406377Z"
        },
        "id": "infrared-cross"
      },
      "source": [
        "model.apply(initialize_weights);"
      ],
      "id": "infrared-cross",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:10.431862Z",
          "start_time": "2021-03-11T09:43:10.420344Z"
        },
        "id": "blessed-photograph"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "id": "blessed-photograph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:10.444738Z",
          "start_time": "2021-03-11T09:43:10.433637Z"
        },
        "id": "neither-month"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "#criterion = nn.CTCLoss()"
      ],
      "id": "neither-month",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T09:43:18.820312Z",
          "start_time": "2021-03-11T09:43:10.447217Z"
        },
        "scrolled": true,
        "id": "complicated-seller",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf20062b-7bc7-4bf7-8655-fbec3a949ef5"
      },
      "source": [
        "N_EPOCHS = 40\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        patience=0\n",
        "    elif patience<5:\n",
        "        patience+=1\n",
        "    else:\n",
        "        print(f\"Early stopping as model's validation loss failed to improve beyond {best_valid_loss}\")\n",
        "        break\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "id": "complicated-seller",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 8s\n",
            "\tTrain Loss: 4.210 | Train PPL:  67.381\n",
            "\t Val. Loss: 2.830 |  Val. PPL:  16.940\n",
            "Epoch: 02 | Time: 0m 8s\n",
            "\tTrain Loss: 2.777 | Train PPL:  16.078\n",
            "\t Val. Loss: 2.392 |  Val. PPL:  10.931\n",
            "Epoch: 03 | Time: 0m 8s\n",
            "\tTrain Loss: 2.408 | Train PPL:  11.115\n",
            "\t Val. Loss: 2.131 |  Val. PPL:   8.423\n",
            "Epoch: 04 | Time: 0m 8s\n",
            "\tTrain Loss: 2.162 | Train PPL:   8.690\n",
            "\t Val. Loss: 1.975 |  Val. PPL:   7.208\n",
            "Epoch: 05 | Time: 0m 8s\n",
            "\tTrain Loss: 1.972 | Train PPL:   7.185\n",
            "\t Val. Loss: 1.867 |  Val. PPL:   6.467\n",
            "Epoch: 06 | Time: 0m 8s\n",
            "\tTrain Loss: 1.815 | Train PPL:   6.142\n",
            "\t Val. Loss: 1.751 |  Val. PPL:   5.760\n",
            "Epoch: 07 | Time: 0m 8s\n",
            "\tTrain Loss: 1.672 | Train PPL:   5.324\n",
            "\t Val. Loss: 1.684 |  Val. PPL:   5.388\n",
            "Epoch: 08 | Time: 0m 8s\n",
            "\tTrain Loss: 1.550 | Train PPL:   4.710\n",
            "\t Val. Loss: 1.592 |  Val. PPL:   4.914\n",
            "Epoch: 09 | Time: 0m 8s\n",
            "\tTrain Loss: 1.438 | Train PPL:   4.212\n",
            "\t Val. Loss: 1.556 |  Val. PPL:   4.741\n",
            "Epoch: 10 | Time: 0m 8s\n",
            "\tTrain Loss: 1.336 | Train PPL:   3.806\n",
            "\t Val. Loss: 1.492 |  Val. PPL:   4.446\n",
            "Epoch: 11 | Time: 0m 8s\n",
            "\tTrain Loss: 1.242 | Train PPL:   3.464\n",
            "\t Val. Loss: 1.455 |  Val. PPL:   4.284\n",
            "Epoch: 12 | Time: 0m 8s\n",
            "\tTrain Loss: 1.164 | Train PPL:   3.202\n",
            "\t Val. Loss: 1.378 |  Val. PPL:   3.965\n",
            "Epoch: 13 | Time: 0m 8s\n",
            "\tTrain Loss: 1.085 | Train PPL:   2.959\n",
            "\t Val. Loss: 1.367 |  Val. PPL:   3.922\n",
            "Epoch: 14 | Time: 0m 8s\n",
            "\tTrain Loss: 1.022 | Train PPL:   2.777\n",
            "\t Val. Loss: 1.317 |  Val. PPL:   3.730\n",
            "Epoch: 15 | Time: 0m 8s\n",
            "\tTrain Loss: 0.963 | Train PPL:   2.619\n",
            "\t Val. Loss: 1.308 |  Val. PPL:   3.700\n",
            "Epoch: 16 | Time: 0m 8s\n",
            "\tTrain Loss: 0.914 | Train PPL:   2.493\n",
            "\t Val. Loss: 1.296 |  Val. PPL:   3.654\n",
            "Epoch: 17 | Time: 0m 8s\n",
            "\tTrain Loss: 0.869 | Train PPL:   2.384\n",
            "\t Val. Loss: 1.286 |  Val. PPL:   3.618\n",
            "Epoch: 18 | Time: 0m 8s\n",
            "\tTrain Loss: 0.819 | Train PPL:   2.268\n",
            "\t Val. Loss: 1.271 |  Val. PPL:   3.565\n",
            "Epoch: 19 | Time: 0m 8s\n",
            "\tTrain Loss: 0.788 | Train PPL:   2.200\n",
            "\t Val. Loss: 1.255 |  Val. PPL:   3.509\n",
            "Epoch: 20 | Time: 0m 8s\n",
            "\tTrain Loss: 0.751 | Train PPL:   2.120\n",
            "\t Val. Loss: 1.256 |  Val. PPL:   3.512\n",
            "Epoch: 21 | Time: 0m 8s\n",
            "\tTrain Loss: 0.720 | Train PPL:   2.055\n",
            "\t Val. Loss: 1.237 |  Val. PPL:   3.445\n",
            "Epoch: 22 | Time: 0m 8s\n",
            "\tTrain Loss: 0.690 | Train PPL:   1.994\n",
            "\t Val. Loss: 1.253 |  Val. PPL:   3.501\n",
            "Epoch: 23 | Time: 0m 8s\n",
            "\tTrain Loss: 0.663 | Train PPL:   1.941\n",
            "\t Val. Loss: 1.244 |  Val. PPL:   3.468\n",
            "Epoch: 24 | Time: 0m 8s\n",
            "\tTrain Loss: 0.640 | Train PPL:   1.896\n",
            "\t Val. Loss: 1.226 |  Val. PPL:   3.407\n",
            "Epoch: 25 | Time: 0m 8s\n",
            "\tTrain Loss: 0.613 | Train PPL:   1.846\n",
            "\t Val. Loss: 1.242 |  Val. PPL:   3.463\n",
            "Epoch: 26 | Time: 0m 8s\n",
            "\tTrain Loss: 0.594 | Train PPL:   1.811\n",
            "\t Val. Loss: 1.254 |  Val. PPL:   3.504\n",
            "Epoch: 27 | Time: 0m 8s\n",
            "\tTrain Loss: 0.573 | Train PPL:   1.774\n",
            "\t Val. Loss: 1.258 |  Val. PPL:   3.517\n",
            "Epoch: 28 | Time: 0m 8s\n",
            "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
            "\t Val. Loss: 1.263 |  Val. PPL:   3.537\n",
            "Epoch: 29 | Time: 0m 8s\n",
            "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
            "\t Val. Loss: 1.266 |  Val. PPL:   3.546\n",
            "Early stopping as model's validation loss failed to improve beyond 1.2258817979267664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdNF6lbwx1Pv"
      },
      "source": [
        "!cp best_model.pt drive/MyDrive/NLP/EngToPython/."
      ],
      "id": "bdNF6lbwx1Pv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOCOdtzEBHjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e50c92a-26c7-41f3-87b5-dff3f2019493"
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "id": "TOCOdtzEBHjT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.226 | Test PPL:   3.407 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa_Vmjt_-1n7"
      },
      "source": [
        "### Testing Model\n"
      ],
      "id": "qa_Vmjt_-1n7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6qXdGPFBS1B"
      },
      "source": [
        "def translate_to_python(sentence, src_field, trg_field, model, device, max_len = 1000):\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "    if isinstance(sentence, str):\n",
        "        #nlp = spacy.load('en')\n",
        "        tokens = tokenizer_en(sentence)\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "        \n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    \n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    for i in range(max_len):\n",
        "\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        \n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "    \n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "    \n",
        "    return trg_tokens[1:], attention"
      ],
      "id": "F6qXdGPFBS1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS7mHXWD7xZ2"
      },
      "source": [
        "token_type_dict = defaultdict(lambda: [])\n",
        "token_dictionary = dict()\n",
        "for python_block in python_indent_fixed:\n",
        "    text = python_block.expandtabs().strip()\n",
        "    text = io.StringIO(text)\n",
        "    for tok in tokenize.generate_tokens(text.readline):\n",
        "        token_type_dict[tok.type].append(tok.string)\n",
        "        token_dictionary[tok.string] = tok.type\n",
        "        "
      ],
      "id": "ZS7mHXWD7xZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XClzf64Q-_sP"
      },
      "source": [
        "def print_code(code, tok_dict={}):\n",
        "    code_str = ''\n",
        "    num_indents = 0\n",
        "    after_newline = False\n",
        "    prev_special = False\n",
        "    for tok in code:\n",
        "        str_type = tok_dict.get(tok, 1)\n",
        "        if tok=='endmarker':\n",
        "            break\n",
        "        elif tok=='indent':\n",
        "            num_indents+=1\n",
        "        elif tok=='newline':\n",
        "            code_str+='\\n'\n",
        "            after_newline = True\n",
        "        elif tok=='dedent':\n",
        "            num_indents-=1\n",
        "        elif after_newline:\n",
        "            code_str+='    '*num_indents\n",
        "            code_str+=tok\n",
        "            after_newline=False\n",
        "            if prev_special:\n",
        "                prev_special = False\n",
        "            #if str_type==1 or str_type==3:\n",
        "            #    code_str+=' '+tok\n",
        "            #else:\n",
        "            #    code_str+=tok\n",
        "        elif str_type==53:\n",
        "            prev_special = True\n",
        "            code_str+=tok\n",
        "        elif prev_special:\n",
        "            code_str+=tok\n",
        "            prev_special=False\n",
        "        else:\n",
        "            code_str+=' '+tok if len(code_str) > 0 else tok\n",
        "    print(code_str)\n"
      ],
      "id": "XClzf64Q-_sP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HpNAIe39p7c",
        "outputId": "62959fe3-a00e-4a82-fcf4-28563924fdfa"
      },
      "source": [
        "token_dictionary['while']"
      ],
      "id": "4HpNAIe39p7c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfDBJ5Xv0EbO",
        "outputId": "fd70602c-7f7a-4110-ac84-9dcc062b60a6"
      },
      "source": [
        "num=55\n",
        "print(english_sents_cleaned[num])\n",
        "code, _ = translate_to_python(english_sents_cleaned[num], SRC, TRG, model, device, max_len = 1000)\n",
        "print_code(code[:-1], tok_dict=token_dictionary)"
      ],
      "id": "bfDBJ5Xv0EbO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Write a function that takes number of disks in tower of hanaoi problem and returns the minimum number of steps required\n",
            "\n",
            "def knapsack(w,n):\n",
            "    if n==0:\n",
            "        return 0\n",
            "    return 1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8HBA_6E5O5r"
      },
      "source": [
        "#Evaluate Model - BLEU"
      ],
      "id": "e8HBA_6E5O5r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo7GTvm78pir"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 1000):    \n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "    \n",
        "    for datum in data:\n",
        "        \n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "        \n",
        "        pred_trg, _ = translate_to_python(src, src_field, trg_field, model, device, max_len)\n",
        "        \n",
        "        #cut off <eos> token\n",
        "        pred_trg = pred_trg[:-1]\n",
        "        \n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "    \n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    score=0.0\n",
        "    for i,j in zip(trgs,pred_trgs):\n",
        "        score+=sentence_bleu(i,j)\n",
        "    return score/len(trgs)"
      ],
      "id": "lo7GTvm78pir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3zN4o5z5SQp"
      },
      "source": [
        "trgs_list = []\n",
        "preds_list = []\n",
        "pred_tokens = output.argmax(2)\n",
        "trg_field=TRG\n",
        "for j in range(len(pred_tokens)):\n",
        "     #cut off <eos> token and <pad> token\n",
        "    ig1= trg_field.vocab.stoi[trg_field.pad_token]\n",
        "    ig2= trg_field.vocab.stoi[trg_field.eos_token]\n",
        "    pred_list = [trg_field.vocab.itos[i] for i in list(pred_tokens[j]) if ((i!=ig1) and (i!=ig2))]\n",
        "    trg_list = [trg_field.vocab.itos[i] for i in list(trg[j,1:]) if ((i!=ig1) and (i!=ig2))]\n",
        "    preds_list.append(pred_list)\n",
        "    trgs_list.append(trg_list)\n",
        "bleu_score(preds_list, trgs_list) "
      ],
      "id": "D3zN4o5z5SQp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKj2ceas8smH",
        "outputId": "aea8e594-2b48-4ded-ebf9-41afff26bb08"
      },
      "source": [
        "bleu_score = calculate_bleu(valid_ds, SRC, TRG, model, device)\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "id": "iKj2ceas8smH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 38.47\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}